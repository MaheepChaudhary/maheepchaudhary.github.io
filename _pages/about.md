---
permalink: /
title: <p>AI Safety Researcher <span>|</span> Mechanistic Interpretability <span>|</span> LLM Safety</p>
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

## About Me

<span style="font-size: 1.0em">I'm an AI safety researcher focused on developing methods to detect harmful outputs and understand the internal mechanisms of large language models. Currently collaborating with researchers at Oxford, Stanford, and other leading institutions on foundational problems in AI alignment and interpretability.</span>

<span style="font-size: 0.95em">My work bridges theoretical foundations with practical safety applications‚Äîfrom publishing causal abstraction theory in JMLR to developing SafetyNet, a system for detecting deceptive behaviors in LLMs (under review at NeurIPS 2025).</span>

---

## Current Research Highlights

**üî¨ SafetyNet: LLM Harm Detection** (Oxford, 2024-25)  
<span style="font-size: 0.9em">Developing novel approaches to detect harmful outputs by modeling deceptive behaviors in large language models. Under review at NeurIPS 2025.</span>

**üìä Mechanistic Interpretability** (Stanford, 2023-24)  
<span style="font-size: 0.9em">Co-authored foundational work on <a href="#">"Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability"</a> published in JMLR 2024 with <a href="https://atticusg.github.io">Dr. Atticus Geiger</a>.</span>

**üîç Sparse Autoencoders for Knowledge Extraction**  
<span style="font-size: 0.9em">Evaluating methods to disentangle factual knowledge in neural networks, advancing our understanding of how models store and retrieve information.</span>

---

## Recognition & Impact

**üèÜ Research Leadership**
- Winner, Smart India Hackathon (200K+ participants) - Criminal Face Recognition System
- Team Leader, ASEAN-India Hackathon (10+ countries) - Marine Species Detection
- Mentor, UNESCO-India-Africa Program (20+ countries) - Voice Systems for Farmers

**üìù Academic Service**
- Reviewer: ICML 2025 Workshop, NeurIPS 2024 Workshops (MATH+AI, CALM)
- Multi-year collaborations with Stanford, Oxford, UIUC, and IIT Indore

---

## Research Collaborations

<span style="font-size: 0.9em">**Current**: Working with <a href="#">Dr. Fazl Barez</a> (Oxford) on AI safety, <a href="https://atticusg.github.io">Dr. Atticus Geiger</a> (Stanford) on interpretability, and <a href="https://haohanwang.github.io">Prof. Haohan Wang</a> (UIUC) on trustworthy ML.</span>

<span style="font-size: 0.9em">**Previous**: Collaborated with <a href="https://www.ntu.edu.sg/scse/about-us/past-chairs/prof-ong-yew-soon">Prof. Ong Yee Soon</a> (NTU Singapore), the <a href="http://driverless.mit.edu/">MIT Driverless team</a>, and researchers at IIT Indore on various ML and computer vision projects.</span>

---

## Research Philosophy

<span style="font-size: 0.9em">I believe AI systems must be both powerful and safe. My research focuses on understanding how neural networks make decisions and developing practical methods to ensure they behave as intended. Read my full <a href="https://drive.google.com/file/d/1Al37c66ZkPu9T0WxXt1ZcBdLtxxZ6Aha/view?usp=sharing">Research Statement</a> to learn more.</span>

---

<div style="text-align: center; margin-top: 30px;">
<strong>Interested in collaboration or have questions about my work?</strong><br>
<span style="font-size: 0.9em">Feel free to reach out via email or check out my latest publications.</span>
</div>