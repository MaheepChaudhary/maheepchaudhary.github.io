---
permalink: /
title: <p>AI Safety Researcher <span>|</span> Mechanistic Interpretability <span>|</span> LLM Safety</p>
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<div style="max-width: 800px; margin: 0 auto; line-height: 1.6;">

<h2>About Me</h2>

<!-- <p style="font-size: 1.1em; color: #333; margin-bottom: 15px;">I'm on a quest to make AI systems both powerful and safe.</p> -->

<p style="font-size: 1.0em; margin-bottom: 15px;">I'm an AI safety researcher focused on understanding the internal mechanisms of large language models and developing practical methods to detect harmful outputs. Currently collaborating with researchers at <strong>Oxford</strong>, <strong>Stanford</strong>, and other leading institutions on foundational problems in AI alignment and interpretability.</p>

<p style="font-size: 0.95em; color: #666;">My work bridges theoretical foundations with real-world safety applications‚Äîfrom publishing causal abstraction theory in <strong>JMLR</strong> to developing <strong>SafetyNet</strong>, a system for detecting deceptive behaviors in LLMs.</p>

<hr style="margin: 40px 0; border: none; height: 1px; background: #ddd;">

<h2>Current Research</h2>

<p style="font-size: 1.0em; margin-bottom: 30px;">Here's some of my recent work on making AI systems more interpretable and safer:</p>

<div style="margin: 30px 0; padding: 20px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #007acc; display: flex; gap: 20px; align-items: flex-start;">
  <img src="/images/safetynet.png" alt="SafetyNet Research" style="width: 100px; height: 130px; border-radius: 8px; flex-shrink: 0; object-fit: cover;">
  <div>
    <h3 style="margin: 0 0 10px 0; color: #007acc;">SafetyNet: Detecting Harmful Outputs in LLMs by Modeling Deceptive Behaviors</h3>
    <p style="margin: 0 0 10px 0; font-size: 0.9em; color: #666;">Maheep Chaudhary, Fazl Barez (Oxford, 2024-25)</p>
    <p style="margin: 0; font-size: 0.95em;">Developing novel approaches to detect when large language models produce harmful content by understanding their internal deceptive mechanisms. <strong>Under review at NeurIPS 2025.</strong></p>
  </div>
</div>

<div style="margin: 30px 0; padding: 20px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #28a745; display: flex; gap: 20px; align-items: flex-start;">
  <img src="/images/causal_abstraction.png" alt="Causal Abstraction Research" style="width: 100px; height: 120px; border-radius: 8px; flex-shrink: 0; object-fit: cover;">
  <div>
    <h3 style="margin: 0 0 10px 0; color: #28a745;">Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability</h3>
    <p style="margin: 0 0 10px 0; font-size: 0.9em; color: #666;">Atticus Geiger, Daniel Ibeling, ..., Maheep Chaudhary, ..., Christopher Potts (Stanford, 2023-24)</p>
    <p style="margin: 0; font-size: 0.95em;">Co-authored foundational theoretical work that provides rigorous mathematical frameworks for understanding how neural networks make decisions. <strong>Published in JMLR 2024.</strong></p>
  </div>
</div>

<div style="margin: 30px 0; padding: 20px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #ffc107; display: flex; gap: 20px; align-items: flex-start;">
  <img src="/images/evaluation_sae.png" alt="Sparse Autoencoder Research" style="width: 100px; height: 70px; border-radius: 8px; flex-shrink: 0; object-fit: cover;">
  <div>
    <h3 style="margin: 0 0 10px 0; color: #e67e22;">Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge</h3>
    <p style="margin: 0 0 10px 0; font-size: 0.9em; color: #666;">Maheep Chaudhary, Atticus Geiger (Stanford, 2024)</p>
    <p style="margin: 0; font-size: 0.95em;">Systematic evaluation of methods to extract and understand how language models store factual knowledge, advancing our ability to interpret AI decision-making. <strong>Under review.</strong></p>
  </div>
</div>

<div style="margin: 30px 0; padding: 20px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #dc3545; display: flex; gap: 20px; align-items: flex-start;">
  <img src="/images/modularity.png" alt="Modular Training Research" style="width: 100px; height: 50px; border-radius: 8px; flex-shrink: 0; object-fit: cover;">
  <div>
    <h3 style="margin: 0 0 10px 0; color: #dc3545;">Modular Training of Neural Networks aids Interpretability</h3>
    <p style="margin: 0 0 10px 0; font-size: 0.9em; color: #666;">Satvik Golechha, Maheep Chaudhary, Joan Velja, Alessandro Abate, Nandi Schoots</p>
    <p style="margin: 0; font-size: 0.95em;">Exploring how training neural networks in modular components makes them more interpretable and trustworthy. <strong>Under review.</strong></p>
  </div>
</div>

<hr style="margin: 40px 0; border: none; height: 1px; background: #ddd;">

<h2>Recognition & Impact</h2>

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
  <div>
    <h4 style="color: #007acc; margin-bottom: 10px;">üèÜ Research Leadership</h4>
    <ul style="font-size: 0.9em; line-height: 1.4;">
      <li><strong>Winner</strong>, Smart India Hackathon (200K+ participants)</li>
      <li><strong>Team Leader</strong>, ASEAN-India Hackathon (10+ countries)</li>
      <li><strong>Mentor</strong>, UNESCO-India-Africa Program (20+ countries)</li>
    </ul>
  </div>
  <div>
    <h4 style="color: #28a745; margin-bottom: 10px;">üìù Academic Service</h4>
    <ul style="font-size: 0.9em; line-height: 1.4;">
      <li>Reviewer: ICML 2025 Workshop</li>
      <li>Reviewer: NeurIPS 2024 Workshops</li>
      <li>Multi-year collaborations with top institutions</li>
    </ul>
  </div>
</div>

<hr style="margin: 40px 0; border: none; height: 1px; background: #ddd;">

<h2>Research Philosophy</h2>

<div style="padding: 25px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 12px; margin: 30px 0;">
  <h3 style="margin: 0 0 15px 0; color: white;">Making AI Safe Through Understanding</h3>
  <p style="margin: 0; font-size: 1.0em; line-height: 1.5;">I believe AI systems must be both powerful and safe. My research focuses on <strong>understanding how neural networks make decisions</strong> and developing <strong>practical methods to ensure they behave as intended</strong>. From theoretical foundations to deployed safety systems, I work across the full spectrum of AI alignment research.</p>
</div>

<p style="font-size: 0.9em; color: #666;">Want to know more about my approach? Read my full <a href="https://drive.google.com/file/d/1Al37c66ZkPu9T0WxXt1ZcBdLtxxZ6Aha/view?usp=sharing" style="color: #007acc;">Research Statement</a>.</p>

<hr style="margin: 40px 0; border: none; height: 1px; background: #ddd;">

<h2>Collaborations</h2>

<p><strong>Current</strong>: Working with <a href="#" style="color: #007acc;">Dr. Fazl Barez</a> (Oxford) on AI safety, <a href="https://atticusg.github.io" style="color: #007acc;">Dr. Atticus Geiger</a> (Stanford) on interpretability, and <a href="https://haohanwang.github.io" style="color: #007acc;">Prof. Haohan Wang</a> (UIUC) on trustworthy ML.</p>

<p><strong>Previous</strong>: Collaborated with <a href="https://www.ntu.edu.sg/scse/about-us/past-chairs/prof-ong-yew-soon" style="color: #007acc;">Prof. Ong Yee Soon</a> (NTU Singapore), the <a href="http://driverless.mit.edu/" style="color: #007acc;">MIT Driverless team</a>, and researchers at IIT Indore.</p>

<hr style="margin: 40px 0; border: none; height: 1px; background: #ddd;">

<div style="text-align: center; padding: 30px; background: #f8f9fa; border-radius: 12px; margin: 30px 0;">
  <h3 style="margin: 0 0 15px 0; color: #333;">Interested in collaboration?</h3>
  <p style="margin: 0; font-size: 0.95em; color: #666;">Drop me an email to discuss research, AI safety, or potential collaborations!</p>
</div>

</div>

<style>
/* Custom styling for dynamic text effects */
h2, h3 {
  font-weight: 600;
  letter-spacing: -0.5px;
}

a {
  text-decoration: none;
  border-bottom: 2px solid transparent;
  transition: border-bottom 0.3s ease;
}

a:hover {
  border-bottom: 2px solid #007acc;
}

.research-card {
  transition: transform 0.3s ease, box-shadow 0.3s ease;
}

.research-card:hover {
  transform: translateY(-5px);
  box-shadow: 0 10px 25px rgba(0,0,0,0.1);
}
</style>